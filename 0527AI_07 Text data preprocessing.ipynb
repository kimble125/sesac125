{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b85d5bc",
   "metadata": {},
   "source": [
    "# 데이터 분석7. 텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefc91d",
   "metadata": {},
   "source": [
    "## Ⅰ 텍스트 전처리란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913926f",
   "metadata": {},
   "source": [
    "텍스트 데이터 전처리는 인공지능과 데이터 분석에서 필수적인 단계로, 데이터의 품질을 높여 모델의 성능을 극대화한다. 이 과정에는 불필요한 문자나 단어 제거, 단어의 형태 통일, 형태소 분석 등이 포함된다. 그리고 이를 통해 데이터 품질을 향상시키고, 자연어처리 작업의 정확도와 효율성을 높이며 워드클라우드와 같은 시각화로도 활용될 수 있다.\n",
    "\n",
    "전처리에는 정제(Cleaning), 정규화(Normalization), 정규식(Regular Expression), 불용어 제거, 형태소 분석 등이 포함된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ead07",
   "metadata": {},
   "source": [
    "## ⅠⅠ 개념과 연습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f2d86",
   "metadata": {},
   "source": [
    "## 1. 텍스트 정제(Cleaning)\n",
    "텍스트 정제는 원시 텍스트에서 분석에 방해가 되는 불필요한 부분(노이즈)를 제거하는 작업이며 다음과 같은 작업들이 포함된다.\n",
    "\n",
    "- 특수문자, HTML 태그, 불필요한 공백, 숫자 등 의미 없는 요소들을 제거하는 작업. \n",
    "\n",
    "- 등장 빈도가 매우 낮거나 너무 짧은은 단어 제거\n",
    "  *한국어의 특성상 한 글자 단어가 많으므로 단순히 길이를 기준으로 제거하는 데는 주의 필요\n",
    "\n",
    "- 의미가 거의 없고 자주 등장하는 조사, 접속사 등 불용어(stopwords)를 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfbbf8",
   "metadata": {},
   "source": [
    "### 1-1. 노이즈 제거 함수 (특수문자, 숫자 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e192014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: 안녕하세요! This is a test... 전화번호는 010-1234-5678 입니다.\n",
      "정제 후: 안녕하세요 This is a test 전화번호는  입니다\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r'[^가-힣a-zA-Z\\s]', '', text)\n",
    "    return cleaned\n",
    "    # 한글과 영어, 공백만 남기기\n",
    "    # ^ : 대괄호 안에서 맨 앞에 쓰이면 부정(not) 의미. 즉, 뒤에 나오는 문자들을 제외한 나머지.\n",
    "    # 가-힣 : 한글 음절 범위를 의미해. 한글의 모든 완성형 글자들이 이 범위에 포함됨.\n",
    "    # a-z : 영어 소문자 a부터 z까지\n",
    "    # A-Z : 영어 대문자 A부터 Z까지\n",
    "    # \\s : 공백 문자(스페이스, 탭, 줄바꿈 등)\n",
    "\n",
    "sample_text = \"안녕하세요! This is a test... 전화번호는 010-1234-5678 입니다.\"\n",
    "print(\"원본:\", sample_text)\n",
    "print(\"정제 후:\", clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b128151",
   "metadata": {},
   "source": [
    "### 1-2. 등장 빈도 적은 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98459f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 토큰: ['서울', '서울', '부산', '대구', '서울', '광주', '대구', '포항']\n",
      "빈도 2 이상 단어만: ['서울', '서울', '대구', '서울', '대구']\n"
     ]
    }
   ],
   "source": [
    "def remove_rare_words(tokens, min_freq=2):\n",
    "    freq = Counter(tokens)\n",
    "    filtered = [w for w in tokens if freq[w] >= min_freq]\n",
    "    return filtered\n",
    "\n",
    "tokens = [\"서울\", \"서울\", \"부산\", \"대구\", \"서울\", \"광주\", \"대구\", \"포항\"]\n",
    "print(\"원본 토큰:\", tokens)\n",
    "print(\"빈도 2 이상 단어만:\", remove_rare_words(tokens, min_freq=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8e5d4",
   "metadata": {},
   "source": [
    "### * 한국어에서 짧은 단어 제거 주의점  \n",
    "한 글자 단어가 의미 있는 경우가 많으므로 무조건 제거하면 중요한 단어가 사라진다.  \n",
    "ex) '나', '너', '가' 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dcdbf7",
   "metadata": {},
   "source": [
    "### 1-3. 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f7ee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전: ['나는', '오늘', '서울', '에', '갔습니다', '그리고', '친구', '와', '만났습니다']\n",
      "불용어 제거 후: ['나는', '오늘', '서울', '갔습니다', '그리고', '친구', '만났습니다']\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['의', '가', '이', '은', '들', '는', '을', '를', '에', '와', '과', '도']\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [w for w in tokens if w not in stopwords]\n",
    "\n",
    "sample_tokens = [\"나는\", \"오늘\", \"서울\", \"에\", \"갔습니다\", \"그리고\", \"친구\", \"와\", \"만났습니다\"]\n",
    "print(\"불용어 제거 전:\", sample_tokens)\n",
    "print(\"불용어 제거 후:\", remove_stopwords(sample_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d5ba1",
   "metadata": {},
   "source": [
    "## 2. 텍스트 정규화(Normalization)\n",
    "정규화는 다양한 표현으로 나타나는 단어를 통일하는 작업이다. \n",
    "\n",
    "\n",
    "- '서울', '서울시', '서울특별시'를 모두 '서울'로 변경 \n",
    "- 대소문자를 일괄적으로 맞추는 작업\n",
    "-  표제어 추출(lemmatization, 사전에 등재된 기본형으으로 변환)과 어간 추출(stemming, 단어의 접사를 잘라내어 어간만 남김)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cc238",
   "metadata": {},
   "source": [
    "### 2-1.규칙 기반 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a13ddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "규칙 기반 정규화: ['USA', 'USA', '서울', '서울', '부산']\n"
     ]
    }
   ],
   "source": [
    "def normalize_word(word):\n",
    "    mapping = {\n",
    "        \"US\": \"USA\",\n",
    "        \"서울시\": \"서울\",\n",
    "        \"서울특별시\": \"서울\"\n",
    "    }\n",
    "    return mapping.get(word, word)\n",
    "\n",
    "words = [\"US\", \"USA\", \"서울시\", \"서울특별시\", \"부산\"]\n",
    "normalized_words = [normalize_word(w) for w in words]\n",
    "print(\"규칙 기반 정규화:\", normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec0ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 전: 나는 서울특별시에 살고 있고, 그렇지만 US를 매우 좋아합니다.\n",
      "정규화 후: 나는 서울특별시에 살고 있고 그렇지만 US를 매우 좋아합니다\n"
     ]
    }
   ],
   "source": [
    "# 문장 간단화 예시\n",
    "sentence = \"나는 서울특별시에 살고 있고, 그렇지만 US를 매우 좋아합니다.\"\n",
    "# 단순 띄어쓰기 기준 분리 (실제 NLP에서는 형태소 분석 필요)\n",
    "sentence_words = sentence.replace(\",\", \"\").replace(\".\", \"\").split()\n",
    "normalized_sentence_words = [normalize_word(w) for w in sentence_words]\n",
    "normalized_sentence = ' '.join(normalized_sentence_words)\n",
    "print(\"정규화 전:\", sentence)\n",
    "print(\"정규화 후:\", normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9c6d4",
   "metadata": {},
   "source": [
    "띄어쓰기가 안 된 단어들은 정규화 인식을 하지 못한다. 실제 NLP에서는 형태소 분석이 필요하지만, 지금은 더욱 정밀한 정규화를 시도해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1c33c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 전: 나는 서울특별시에 살고 있고, 그렇지만 US를 매우 좋아합니다.\n",
      "정규화 후: 나는 서울에 살고 있고 그렇지만 USA를 매우 좋아합니다\n"
     ]
    }
   ],
   "source": [
    "def normalize_word(word):\n",
    "    mapping = {\n",
    "        \"US\": \"USA\",\n",
    "        \"서울시\": \"서울\",\n",
    "        \"서울특별시\": \"서울\"\n",
    "    }\n",
    "    for k, v in mapping.items():\n",
    "        word = word.replace(k, v)\n",
    "    return word\n",
    "# mapping.items()는 딕셔너리의 (키, 값) 쌍을 하나씩 꺼낸다. \n",
    "# word.replace(k, v)는 word 문자열에서 k를 찾아 모두 v로 바꾼다. (실제 NLP에서는 형태소 분석 필요)\n",
    "\n",
    "sentence = \"나는 서울특별시에 살고 있고, 그렇지만 US를 매우 좋아합니다.\"\n",
    "sentence_words = sentence.replace(\",\", \"\").replace(\".\", \"\").split()\n",
    "normalized_sentence_words = [normalize_word(w) for w in sentence_words]\n",
    "normalized_sentence = ' '.join(normalized_sentence_words)\n",
    "print(\"정규화 전:\", sentence)\n",
    "print(\"정규화 후:\", normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef71bf0",
   "metadata": {},
   "source": [
    "### 2-2. 대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b2a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대소문자 통합: ['apple', 'apple', 'apple', 'banana']\n"
     ]
    }
   ],
   "source": [
    "sample_english = [\"Apple\", \"apple\", \"APPLE\", \"Banana\"]\n",
    "lowercase_words = [w.lower() for w in sample_english]\n",
    "print(\"대소문자 통합:\", lowercase_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b73686",
   "metadata": {},
   "source": [
    "### 2-3. Lemmatization vs Stemming \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59779fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\el76\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\el76\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\el76\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\el76\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\el76\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\el76\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246d3d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\EL76\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EL76\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "표제어 추출 결과:\n",
      "am -> be\n",
      "are -> be\n",
      "is -> be\n",
      "having -> have\n",
      "runs -> run\n",
      "어간 추출 결과:\n",
      "am -> am\n",
      "are -> are\n",
      "is -> is\n",
      "having -> have\n",
      "runs -> run\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"am\", \"are\", \"is\", \"having\", \"runs\"]\n",
    "\n",
    "print(\"\\n표제어 추출 결과:\")\n",
    "for w in words:\n",
    "    print(f\"{w} -> {lemmatizer.lemmatize(w, pos='v')}\")\n",
    "\n",
    "print(\"어간 추출 결과:\")\n",
    "for w in words:\n",
    "    print(f\"{w} -> {ps.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86873db1",
   "metadata": {},
   "source": [
    "## 3. 정규식(Regular Expression)을 이용한 패턴 제거\n",
    "정규식은 텍스트에서 특정한 패턴을 찾아내거나 제거하는 데 유용하다. 예를 들어, 전화번호 형식, 이메일 주소, HTML 태그 등을 정규식으로 필터링하여 텍스트를 더 깔끔하게 다듬을 수 있다.\n",
    "\n",
    "파이썬의 re 라이브러리는 이러한 정규식을 지원하며, 복잡한 텍스트 전처리에 자주 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6525e",
   "metadata": {},
   "source": [
    "### *정규화와 정규식의 관계\n",
    "- 정규식은 정규화 작업을 수행하는 데 자주 활용된다.\n",
    " 예를 들어, 정규식을 사용해 문장에서 '서울시'나 '서울특별시'라는 패턴을 찾아 모두 '서울'로 바꾼다면, 이 작업은 정규식을 활용한 정규화라고 할 수 있다\n",
    "\n",
    "- 한편, 정규화는 정규식 이외에에 단순 치환, 소문자 변환, 표제어 추출 등 다양한 방법으로 이뤄질 수 있다. \n",
    "\n",
    "- 물론 정규식은 정규화 이외의 데이터 추출과 분리 등의 작업에도 이용될 수 있다. 즉 **정규화**는 '표현 통일'이라는 *목적*에 해당하고, **정규식**은 '패턴 탐색 및 변환'이라는 *도구*에 해당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2c06a",
   "metadata": {},
   "source": [
    "### 3-1. XML, HTML 등의 태그 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cde1a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 HTML: \n",
      "<html>\n",
      "<head><title>테스트</title></head>\n",
      "<body><p>안녕하세요!</p><br/><a href=\"link\">링크</a></body>\n",
      "</html>\n",
      "\n",
      "태그 제거 후: \n",
      "\n",
      "테스트\n",
      "안녕하세요!링크\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(text):\n",
    "    # Non-greedy 매칭으로 태그 제거\n",
    "    cleaned = re.sub(r'<.*?>', '', text, flags=re.DOTALL)\n",
    "    return cleaned\n",
    "\n",
    "html_text = \"\"\"\n",
    "<html>\n",
    "<head><title>테스트</title></head>\n",
    "<body><p>안녕하세요!</p><br/><a href=\"link\">링크</a></body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"원본 HTML:\", html_text)\n",
    "print(\"태그 제거 후:\", remove_html_tags(html_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2350f",
   "metadata": {},
   "source": [
    "### 3-2. 다양한 전화번호 형식을 xxx-xxxx-xxxx로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f6e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트: 연락처: 01012345678, 010-1234-5678, 010 1234 5678\n",
      "정규화 후: 연락처: 010-1234-5678, 010-1234-5678, 010-1234-5678\n"
     ]
    }
   ],
   "source": [
    "def normalize_phone_numbers(text):\n",
    "    # 다양한 형태의 전화번호 패턴을 그룹으로 잡아서 -로 구분된 형태로 변환\n",
    "    pattern = r'(01[016789])[ -]?(\\d{3,4})[ -]?(\\d{4})'\n",
    "    normalized = re.sub(pattern, r'\\1-\\2-\\3', text)\n",
    "    return normalized\n",
    "\n",
    "sample_text = \"연락처: 01012345678, 010-1234-5678, 010 1234 5678\"\n",
    "\n",
    "print(\"원본 텍스트:\", sample_text)\n",
    "print(\"정규화 후:\", normalize_phone_numbers(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611d221",
   "metadata": {},
   "source": [
    "### 3-3. Greedy, Non-Greedy (Back-tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c11e8e",
   "metadata": {},
   "source": [
    "- Greedy (탐욕적 방식): 정규 표현식에서 가능한 가장 긴 문자열을 매칭하려는 방식. 즉, 일치하는 문자열 중에서 최대한 많은 문자를 포함하려고 시도\n",
    "  - 정규식: a.*a\n",
    "      - . : 아무 문자 1개\n",
    "      - * : 앞의 문자가 0개 이상 반복 (Greedy하게 최대한 반복)\n",
    "- Non-Greedy (Lazy): 가능한 가장 짧은 문자열만 매칭하려고 하는 방식. ?를 추가하여 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be7a45ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "aaba\n",
      "a\n",
      "aa\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(re.match(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma*?a\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maaba\u001b[39m\u001b[33m\"\u001b[39m).group())    \u001b[38;5;66;03m# Non-Greedy\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(re.match(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma.*?a\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maaba\u001b[39m\u001b[33m\"\u001b[39m).group())   \u001b[38;5;66;03m# Non-Greedy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma*+a\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maaba\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m())    \u001b[38;5;66;03m# Possessive quantifier (파이썬 re에서는 지원 X)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "print(re.match(r\"a*a\", \"aaba\").group())     # Greedy1: a를 0번 이상 반복 + 그 다음 'a'들들(이후에는b이므로 표기x)\n",
    "print(re.match(r\"a.*a\", \"aaba\").group())    # Greedy2: a를 0번 이상 반복 + 그 다음 'a'까지의 모든 문자\n",
    "\n",
    "print(re.match(r\"a*?a\", \"aaba\").group())    # Non-Greedy\n",
    "print(re.match(r\"a.*?a\", \"aaba\").group())   # Non-Greedy\n",
    "print(re.match(r\"a*+a\", \"aaba\").group())    # Possessive quantifier (파이썬 re에서는 지원 X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f5497",
   "metadata": {},
   "source": [
    "### 3-4. (?:...) 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5d3aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "match = re.match(r'(hello) (\\w+)', 'hello world')\n",
    "print(match.group(1))  # hello (저장됨)\n",
    "print(match.group(2))  # world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f3133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "match = re.match(r'(?:hello) (\\w+)', 'hello world')\n",
    "\n",
    "# match.group(1)은 이제 (\\w+)에 대응되며 (?:hello)는 저장하지 않음\n",
    "print(match.group(1))  # world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d6d56",
   "metadata": {},
   "source": [
    "## 4. 한국어 형태소 분석\n",
    "한국어는 영어와 달리 조사, 어미 등이 붙어 단어의 형태가 다양하게 변하기 때문에, 형태소 분석기가 꼭 필요하다. 형태소 분석기는 텍스트를 형태소 단위로 분리하고 품사 태깅을 수행한다.\n",
    "\n",
    "주요 한국어 형태소 분석기에는 Okt, Mecab, Komoran, Kkma, Hannanum 등이 있으며, 파이썬에서는 KoNLPy 라이브러리를 통해 이들을 활용할 수 있다.\n",
    "\n",
    "예를 들어 Okt는 간단한 문장 형태소 분석에 적합하며, 아래와 같이 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b194fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\el76\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\el76\\anaconda3\\lib\\site-packages (from konlpy) (1.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\el76\\anaconda3\\lib\\site-packages (from konlpy) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\el76\\anaconda3\\lib\\site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\el76\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0306115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\el76\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\el76\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from konlpy) (1.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\el76\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from konlpy) (5.4.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\el76\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from konlpy) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\el76\\appdata\\roaming\\python\\python313\\site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7904e9e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJVMNotFoundException\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkonlpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Okt\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m okt = \u001b[43mOkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33m이것도 되나욬ㅋㅋ\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1) 기본 품사 태깅 (norm, stem 없음)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EL76\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\konlpy\\tag\\_okt.py:51\u001b[39m, in \u001b[36mOkt.__init__\u001b[39m\u001b[34m(self, jvmpath, max_heap_size)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jvmpath=\u001b[38;5;28;01mNone\u001b[39;00m, max_heap_size=\u001b[32m1024\u001b[39m):\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jpype.isJVMStarted():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[43mjvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     oktJavaPackage = jpype.JPackage(\u001b[33m'\u001b[39m\u001b[33mkr.lucypark.okt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     54\u001b[39m     OktInterfaceJavaClass = oktJavaPackage.OktInterface\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EL76\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\konlpy\\jvm.py:55\u001b[39m, in \u001b[36minit_jvm\u001b[39m\u001b[34m(jvmpath, max_heap_size)\u001b[39m\n\u001b[32m     52\u001b[39m args = [javadir, os.sep]\n\u001b[32m     53\u001b[39m classpath = [f.format(*args) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m folder_suffix]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m jvmpath = jvmpath \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mjpype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetDefaultJVMPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.platform == \u001b[33m'\u001b[39m\u001b[33mdarwin\u001b[39m\u001b[33m'\u001b[39m\\\n\u001b[32m     59\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath.find(\u001b[33m'\u001b[39m\u001b[33m1.8.0\u001b[39m\u001b[33m'\u001b[39m) > \u001b[32m0\u001b[39m\\\n\u001b[32m     60\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath.endswith(\u001b[33m'\u001b[39m\u001b[33mlibjvm.dylib\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EL76\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\jpype\\_jvmfinder.py:70\u001b[39m, in \u001b[36mgetDefaultJVMPath\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     69\u001b[39m     finder = LinuxJVMFinder()\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_jvm_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EL76\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\jpype\\_jvmfinder.py:204\u001b[39m, in \u001b[36mJVMFinder.get_jvm_path\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jvm_notsupport_ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m jvm_notsupport_ext\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m JVMNotFoundException(\u001b[33m\"\u001b[39m\u001b[33mNo JVM shared library file (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m                            \u001b[33m\"\u001b[39m\u001b[33mfound. Try setting up the JAVA_HOME \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m                            \u001b[33m\"\u001b[39m\u001b[33menvironment variable properly.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m                            .format(\u001b[38;5;28mself\u001b[39m._libfile))\n",
      "\u001b[31mJVMNotFoundException\u001b[39m: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "text = \"이것도 되나욬ㅋㅋ\"\n",
    "\n",
    "# 1) 기본 품사 태깅 (norm, stem 없음)\n",
    "print(okt.pos(text))\n",
    "# 결과 예:\n",
    "# [('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
    "\n",
    "# 2) 정규화 적용 (norm=True)\n",
    "print(okt.pos(text, norm=True))\n",
    "# [('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
    "\n",
    "# 3) 정규화 + 어간 추출 적용 (norm=True, stem=True)\n",
    "print(okt.pos(text, norm=True, stem=True))\n",
    "# [('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dac468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'terminal.integrated.env.windows': {'JAVA_HOME': 'C:\\\\Program Files\\\\Java\\\\jdk-XX.X.X',\n",
       "  'Path': '%JAVA_HOME%\\\\bin;%Path%'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C:\\Users\\EL76\\OneDrive\\Desktop\\IT\\jdk-24_windows-x64_bin\\jdk-24.0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
