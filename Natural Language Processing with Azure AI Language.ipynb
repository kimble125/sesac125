{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43b3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-textanalytics in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-ai-textanalytics) (1.34.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-ai-textanalytics) (1.1.28)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-ai-textanalytics) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-ai-textanalytics) (4.13.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\el76\\appdata\\roaming\\python\\python313\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8994bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_key = \"9SiYne7CrNU6QY5gQXmLrbhr6ACP7NgwZIdzIco5cpYMziCP1or3JQQJ99BFACYeBjFXJ3w3AAAaACOGiH9z\"\n",
    "language_endpoint = \"https://1sesac125-language.cognitiveservices.azure.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85976d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\EL76\\OneDrive\\Desktop\\IT\\Class\\sesac125\\4. Cloud-Based Machine Learning & Deep Learning\\ML&OpenAI\\AILanguage\\Natural Language Processing with Azure AI Language.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860365b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "\n",
      "\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.66 \tLength: \t 4 \tOffset: \t 18 \n",
      "\n",
      "\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t City \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 7 \tOffset: \t 26 \n",
      "\n",
      "\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 9 \tOffset: \t 34 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities from text\n",
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"I had a wonderful trip to Seattle last week.\"]\n",
    "        result = client.recognize_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Named Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                    \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f8006e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary extracted: \n",
      "It was possible to deploy existing video quality metrics, such as PSNR and SSIM at scale, but they fell short of accurately capturing human perception. Thus, we embarked on a journey to develop an automated way to answer the question, “How will a Netflix member rate the quality of this encode?” Video Multi-method Assessment Fusion, or VMAF for short, is a video quality metric that combines human vision modeling with machine learning. In June 2016, we open-sourced VMAF on Github, and also published the first VMAF techblog.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.ai.textanalytics import ExtractiveSummaryAction\n",
    "\n",
    "    document = [\"\"\"\n",
    "by Zhi Li, Christos Bampis, Julie Novak, Anne Aaron, Kyle Swanson, Anush Moorthy and Jan De Cock\n",
    "\n",
    "How will Netflix members rate the quality of this video — poor, average or excellent?\n",
    "Which video clip looks better — encoded with Codec A or Codec B?\n",
    "For this episode, at 1000 kbps, is it better to encode with HD resolution, with some blockiness, or will SD look better?\n",
    "\n",
    "These were example questions we asked ourselves as we worked towards delivering the best quality of experience for Netflix members. A few years ago, we realized that we were unable to answer these questions effectively by simply relying on “golden eyes.” Expert viewing was not scalable across content, encoding recipes, and the overall output of our encoding pipeline. It was possible to deploy existing video quality metrics, such as PSNR and SSIM at scale, but they fell short of accurately capturing human perception. Thus, we embarked on a journey to develop an automated way to answer the question, “How will a Netflix member rate the quality of this encode?” This was the birth of VMAF.\n",
    "\n",
    "Video Multi-method Assessment Fusion, or VMAF for short, is a video quality metric that combines human vision modeling with machine learning. The project started as research collaboration between our team and Prof. C.-C. Jay Kuo from University of Southern California. His research group had previously worked on perceptual metrics for images, and together, we worked on extending the ideas to video. Over time, we have collaborated with other research partners such as Prof. Alan Bovik from the University of Texas at Austin and Prof. Patrick Le Callet from Université de Nantes with the goal of improving VMAF accuracy related to human subjective perception, and broaden its scope to cover more use cases. In June 2016, we open-sourced VMAF on Github, and also published the first VMAF techblog. In this new techblog, we want to share our journey.\n",
    "\"\"\"]\n",
    "\n",
    "    poller = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for result in document_results:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary extracted: \\n{}\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "sample_extractive_summarization(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b25b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary extracted: \n",
      "It was possible to deploy existing video quality metrics, such as PSNR and SSIM at scale, but they fell short of accurately capturing human perception. Thus, we embarked on a journey to develop an automated way to answer the question, “How will a Netflix member rate the quality of this encode?” Video Multi-method Assessment Fusion, or VMAF for short, is a video quality metric that combines human vision modeling with machine learning. In June 2016, we open-sourced VMAF on Github, and also published the first VMAF techblog.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.ai.textanalytics import ExtractiveSummaryAction\n",
    "\n",
    "    document = [\"\"\"\n",
    "by Zhi Li, Christos Bampis, Julie Novak, Anne Aaron, Kyle Swanson, Anush Moorthy and Jan De Cock\n",
    "\n",
    "How will Netflix members rate the quality of this video — poor, average or excellent?\n",
    "Which video clip looks better — encoded with Codec A or Codec B?\n",
    "For this episode, at 1000 kbps, is it better to encode with HD resolution, with some blockiness, or will SD look better?\n",
    "\n",
    "These were example questions we asked ourselves as we worked towards delivering the best quality of experience for Netflix members. A few years ago, we realized that we were unable to answer these questions effectively by simply relying on “golden eyes.” Expert viewing was not scalable across content, encoding recipes, and the overall output of our encoding pipeline. It was possible to deploy existing video quality metrics, such as PSNR and SSIM at scale, but they fell short of accurately capturing human perception. Thus, we embarked on a journey to develop an automated way to answer the question, “How will a Netflix member rate the quality of this encode?” This was the birth of VMAF.\n",
    "\n",
    "Video Multi-method Assessment Fusion, or VMAF for short, is a video quality metric that combines human vision modeling with machine learning. The project started as research collaboration between our team and Prof. C.-C. Jay Kuo from University of Southern California. His research group had previously worked on perceptual metrics for images, and together, we worked on extending the ideas to video. Over time, we have collaborated with other research partners such as Prof. Alan Bovik from the University of Texas at Austin and Prof. Patrick Le Callet from Université de Nantes with the goal of improving VMAF accuracy related to human subjective perception, and broaden its scope to cover more use cases. In June 2016, we open-sourced VMAF on Github, and also published the first VMAF techblog. In this new techblog, we want to share our journey.\n",
    "\"\"\"]\n",
    "\n",
    "    poller = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for result in document_results:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary extracted: \\n{}\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "sample_extractive_summarization(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3491d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstractive summary extracted: \n",
      "The Netflix team, alongside academic collaborators, sought to develop a method to predict video quality from a member's perspective, moving beyond subjective expert opinions. They aimed to create a metric that accurately reflects human perception, leading to the creation of VMAF—Video Multi-method Assessment Fusion. This metric combines human vision models with machine learning to assess video quality. The project, which started as research collaboration, has since been open-sourced and continues to evolve through further academic partnerships, with the goal of enhancing its accuracy in relation to human perception. The team's journey reflects a commitment to delivering the best viewing experience by grounding quality assessment in a more objective, scalable approach. They share their progress and insights through a dedicated techblog.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.textanalytics import AbstractiveSummaryAction\n",
    "\n",
    "def sample_abstractive_summarization(client):\n",
    "    document = [\"\"\"\n",
    "by Zhi Li, Christos Bampis, Julie Novak, Anne Aaron, Kyle Swanson, Anush Moorthy and Jan De Cock\n",
    "\n",
    "How will Netflix members rate the quality of this video — poor, average or excellent?\n",
    "Which video clip looks better — encoded with Codec A or Codec B?\n",
    "For this episode, at 1000 kbps, is it better to encode with HD resolution, with some blockiness, or will SD look better?\n",
    "\n",
    "These were example questions we asked ourselves as we worked towards delivering the best quality of experience for Netflix members. A few years ago, we realized that we were unable to answer these questions effectively by simply relying on “golden eyes.” Expert viewing was not scalable across content, encoding recipes, and the overall output of our encoding pipeline. It was possible to deploy existing video quality metrics, such as PSNR and SSIM at scale, but they fell short of accurately capturing human perception. Thus, we embarked on a journey to develop an automated way to answer the question, “How will a Netflix member rate the quality of this encode?” This was the birth of VMAF.\n",
    "\n",
    "Video Multi-method Assessment Fusion, or VMAF for short, is a video quality metric that combines human vision modeling with machine learning. The project started as research collaboration between our team and Prof. C.-C. Jay Kuo from University of Southern California. His research group had previously worked on perceptual metrics for images, and together, we worked on extending the ideas to video. Over time, we have collaborated with other research partners such as Prof. Alan Bovik from the University of Texas at Austin and Prof. Patrick Le Callet from Université de Nantes with the goal of improving VMAF accuracy related to human subjective perception, and broaden its scope to cover more use cases. In June 2016, we open-sourced VMAF on Github, and also published the first VMAF techblog. In this new techblog, we want to share our journey.\n",
    "\"\"\"]\n",
    "\n",
    "    poller = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            AbstractiveSummaryAction()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for result in document_results:\n",
    "        abstractive_summary_result = result[0]\n",
    "        if abstractive_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                abstractive_summary_result.code, abstractive_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Abstractive summary extracted: \\n{}\".format(\n",
    "                \"\\n\".join([summary.text for summary in abstractive_summary_result.summaries]))\n",
    "            )\n",
    "\n",
    "sample_abstractive_summarization(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e0547f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-language-questionanswering\n",
      "  Downloading azure_ai_language_questionanswering-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-ai-language-questionanswering) (1.34.0)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-ai-language-questionanswering) (0.7.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\el76\\appdata\\roaming\\python\\python313\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\el76\\anaconda3\\envs\\py313\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (2025.4.26)\n",
      "Downloading azure_ai_language_questionanswering-1.1.0-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: azure-ai-language-questionanswering\n",
      "Successfully installed azure-ai-language-questionanswering-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-language-questionanswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc7a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: negative\n",
      "Overall scores: positive=0.00; neutral=0.00; negative=1.00 \n",
      "\n",
      "Sentence: The food and service were unacceptable. \n",
      "Sentence sentiment: negative\n",
      "Sentence score:\n",
      "Positive=0.00\n",
      "Neutral=0.00\n",
      "Negative=1.00\n",
      "\n",
      "......'negative' target 'food'\n",
      "......Target score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' assessment 'unacceptable'\n",
      "......Assessment score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' target 'service'\n",
      "......Target score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' assessment 'unacceptable'\n",
      "......Assessment score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The concierge was nice, however.\n",
      "Sentence sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.22\n",
      "Neutral=0.75\n",
      "Negative=0.04\n",
      "\n",
      "......'positive' target 'concierge'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'nice'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.questionanswering import QuestionAnsweringClient\n",
    "\n",
    "endpoint = \"https://{YOUR-ENDPOINT}.api.cognitive.microsoft.com/\"\n",
    "credential = AzureKeyCredential(\"{YOUR-LANGUAGE-RESOURCE-KEY}\")\n",
    "knowledge_base_project = \"{YOUR-PROJECT-NAME}\"\n",
    "deployment = \"production\"\n",
    "\n",
    "def main():\n",
    "    client = QuestionAnsweringClient(endpoint, credential)\n",
    "    with client:\n",
    "        question=\"How much battery life do I have left?\"\n",
    "        output = client.get_answers(\n",
    "            question = question,\n",
    "            project_name=knowledge_base_project,\n",
    "            deployment_name=deployment\n",
    "        )\n",
    "    print(\"Q: {}\".format(question))\n",
    "    print(\"A: {}\".format(output.answers[0].answer))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c856130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지는 기술은 마스터하기 어렵지 않습니다. 너무나 많은 것들이 잃어버리려는 의도로 가득 차 있는 것처럼 보이기 때문에 그것들의 상실은 재앙이 아니다.\n"
     ]
    }
   ],
   "source": [
    "# 번역 결과만 깔끔하게 출력\n",
    "for item in response:\n",
    "    for translation in item.get(\"translations\", []):\n",
    "        print(translation[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
